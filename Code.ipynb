{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK9ztcFCt_c1"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8ekNiwVBuKRK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-FsO0UESt08W"
      },
      "outputs": [],
      "source": [
        "class BaselineModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # For 32x32 colour images\n",
        "        self.conv_layer = nn.Conv2d(3, 32, (3, 3))\n",
        "        self.relu = nn.ReLU()\n",
        "        self.max_pool = nn.MaxPool2d((2, 2))\n",
        "        self.flatten = nn.Flatten()\n",
        "        # ((32-2) * (32-2) * 32) / 4 = 7200\n",
        "        self.dense = nn.Linear(7200, 128)\n",
        "        # Relu after dense\n",
        "        self.output = nn.Linear(128, 10) # 10 possible categories\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        x = self.conv_layer(x) # 32x32x3 -> 30x30x32\n",
        "        x = self.relu(x) # 30x30x32 -> 30x30x32\n",
        "        x = self.max_pool(x) # 30x30x32 -> 15x15x32\n",
        "        x = self.flatten(x) # 15x15x32 -> 7200\n",
        "        x = self.dense(x) # 7200 -> 128\n",
        "        x = self.relu(x) # 128 -> 128\n",
        "        x = self.output(x) # 128 -> 10\n",
        "        x = self.softmax(x) # 10 -> 10\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "k7dtYDTK2Vsz"
      },
      "outputs": [],
      "source": [
        "# It has one more convolutional layer than the original model\n",
        "class ExtraConvModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # For 32x32 colour images\n",
        "        self.conv_layer = nn.Conv2d(3, 32, (3, 3))\n",
        "        self.conv_layer_2 = nn.Conv2d(32, 32, (2, 2))\n",
        "        self.relu = nn.ReLU()\n",
        "        self.max_pool = nn.MaxPool2d((2, 2))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dense = nn.Linear(6272, 128)\n",
        "        # Relu after dense\n",
        "        self.output = nn.Linear(128, 10) # 10 possible categories\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        x = self.conv_layer(x) # 32x32x3 -> 30x30x32\n",
        "        x = self.conv_layer_2(x) # 30x30x32 -> 29x29x32\n",
        "        x = self.relu(x)\n",
        "        x = self.max_pool(x) # 14x14x32\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.output(x)\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WkRv8VD-6X04"
      },
      "outputs": [],
      "source": [
        "# It has two more convolutional layers than the original model\n",
        "class ExtraDoubleConvModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # For 32x32 colour images\n",
        "        self.conv_layer = nn.Conv2d(3, 32, (3, 3))\n",
        "        self.conv_layer_2 = nn.Conv2d(32, 64, (4, 4))\n",
        "        self.conv_layer_3 = nn.Conv2d(64, 32, (2, 2))\n",
        "        self.relu = nn.ReLU()\n",
        "        self.max_pool = nn.MaxPool2d((2, 2))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dense = nn.Linear(5408, 128)\n",
        "        # Relu after dense\n",
        "        self.output = nn.Linear(128, 10) # 10 possible categories\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        x = self.conv_layer(x) # 32x32x3 -> 30x30x32\n",
        "        x = self.conv_layer_2(x) # 30x30x32 -> 29x29x32\n",
        "        x = self.conv_layer_3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.max_pool(x) # 14x14x32\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.output(x)\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "42-PcWkuBTGU"
      },
      "outputs": [],
      "source": [
        "# It's like the ExtraConv model, but it has a dropout layer between the two convolutions\n",
        "class DropoutModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # For 32x32 colour images\n",
        "        self.conv_layer = nn.Conv2d(3, 32, (3, 3))\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.conv_layer_2 = nn.Conv2d(32, 32, (2, 2))\n",
        "        self.relu = nn.ReLU()\n",
        "        self.max_pool = nn.MaxPool2d((2, 2))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dense = nn.Linear(6272, 128)\n",
        "        # Relu after dense\n",
        "        self.output = nn.Linear(128, 10) # 10 possible categories\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        x = self.conv_layer(x) # 32x32x3 -> 30x30x32\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv_layer_2(x) # 30x30x32 -> 29x29x32\n",
        "        x = self.relu(x)\n",
        "        x = self.max_pool(x) # 14x14x32\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.output(x)\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcL8MA5iuPBX"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PgfHdDAduSRQ"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "54Q2HrqXuYMI"
      },
      "outputs": [],
      "source": [
        "# To have the data in a more PyTorch-friendly format\n",
        "class CustomCIFAR10Dataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mwBJIVlVuas7"
      },
      "outputs": [],
      "source": [
        "# train_batches -> [batch_num][b'data'][image_num][x][y][rgb]\n",
        "# test_batches -> [image_num][b'data'][x][y][rgb]\n",
        "def load_cifar10_data(data_dir):\n",
        "    train_batches = [unpickle(data_dir + '/data_batch_' + str(i)) for i in range(1, 6)]\n",
        "    for batch in train_batches:\n",
        "        batch[b'data'] = batch[b'data'].reshape((10000, 3, 32, 32))  # Reshape to (num_samples, channels, height, width)\n",
        "        batch[b'data'] = batch[b'data'].astype('float32') / 255.0  # Convert to float and normalize\n",
        "\n",
        "    test_batch = unpickle(data_dir + '/test_batch')\n",
        "    test_batch[b'data'] = test_batch[b'data'].reshape((10000, 3, 32, 32))  # Reshape to (num_samples, channels, height, width)\n",
        "    test_batch[b'data'] = test_batch[b'data'].astype('float32') / 255.0  # Convert to float and normalize\n",
        "\n",
        "    return train_batches, test_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ESHV0uaQucYP"
      },
      "outputs": [],
      "source": [
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "f14y2eCcud5x"
      },
      "outputs": [],
      "source": [
        "def see_sample(train_dataset, index):\n",
        "    image, label = train_dataset[index]\n",
        "\n",
        "    print(f\"Image shape: {image.shape}\")\n",
        "    print(f\"Label: {label}\")\n",
        "\n",
        "    # Visualize the image\n",
        "    plt.imshow(image)  # Permute the dimensions for visualization\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Label: {label}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhHXql1cui9S"
      },
      "source": [
        "# Train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TCxnyQtaufOg"
      },
      "outputs": [],
      "source": [
        "def train(model, train_dataloader, epochs=100, lr=0.001):\n",
        "    # Define the loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # For viewing the training process in console\n",
        "    progress_bar = tqdm(range(epochs), desc=\"Training\", unit=\"epoch\")\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in progress_bar:\n",
        "        for i, (images, labels) in enumerate(train_dataloader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images) # Forward pass\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sFDrsqoqJgF1"
      },
      "outputs": [],
      "source": [
        "def train_with_val(model, train_dataloader, val_dataloader, epochs=100, lr=0.001):\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    progress_bar = tqdm(range(epochs), desc=\"Training\", unit=\"epoch\")\n",
        "    accuracy = None\n",
        "\n",
        "    for epoch in progress_bar:\n",
        "        model.train()\n",
        "        for images, labels in train_dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for images, labels in val_dataloader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "            accuracy = correct / total\n",
        "\n",
        "    print()\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Print the validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EM4U4cAtu9Vl"
      },
      "outputs": [],
      "source": [
        "def test(model, test_dataloader):\n",
        "    # Define the loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize variables to track test accuracy and loss\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    test_loss = 0.0\n",
        "\n",
        "    # For viewing the testing process in console\n",
        "    progress_bar = tqdm(test_dataloader, desc=\"Testing\", unit=\"batch\")\n",
        "\n",
        "    # Disable gradient calculation for testing\n",
        "    with torch.no_grad():\n",
        "        for images, labels in progress_bar:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Update accuracy statistics\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Calculate average test loss and accuracy\n",
        "    avg_test_loss = test_loss / len(test_dataloader)\n",
        "    test_accuracy = correct / total\n",
        "\n",
        "    # Print the test results\n",
        "    print()\n",
        "    print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c1ZowqXvAM6"
      },
      "source": [
        "# **MAIN FUNCTION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pX9GT7GRFe_6"
      },
      "outputs": [],
      "source": [
        "# Get the default training and testing batches given by the dataset itself\n",
        "train_batches, test_batch = load_cifar10_data(\"cifar-10-batches-py\")\n",
        "\n",
        "# Pass the data to a more PyTorch-friendly format\n",
        "train_dataset = CustomCIFAR10Dataset(\n",
        "    torch.cat([torch.tensor(batch[b'data']) for batch in train_batches]),\n",
        "    torch.cat([torch.tensor(batch[b'labels']) for batch in train_batches]))\n",
        "test_dataset = CustomCIFAR10Dataset(test_batch[b'data'], test_batch[b'labels'])\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "# Create custom-sized batches\n",
        "batch_size = 128\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, worker_init_fn=seed_worker)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, worker_init_fn=seed_worker)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8oD1DJ38gNW",
        "outputId": "76814356-69ce-4038-ed71-0acf0b5ecf9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BASE MODEL RESULTS:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 100/100 [02:10<00:00,  1.30s/epoch]\n",
            "Testing: 100%|██████████| 79/79 [00:00<00:00, 446.75batch/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Loss: 1.8228, Test Accuracy: 0.6342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Train and test the base model\n",
        "print(\"BASE MODEL RESULTS:\")\n",
        "model = BaselineModel().to(device)\n",
        "train(model, train_dataloader)\n",
        "test(model, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScXnVp588iVf",
        "outputId": "4a1d9e5c-1a20-46ff-db33-d695e20a1123"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "EXTRA CONVOLUTION MODEL RESULTS:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 100/100 [02:43<00:00,  1.63s/epoch]\n",
            "Testing: 100%|██████████| 79/79 [00:00<00:00, 390.61batch/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Loss: 1.8147, Test Accuracy: 0.6476\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Train and test the extra convolution model\n",
        "print()\n",
        "print(\"EXTRA CONVOLUTION MODEL RESULTS:\")\n",
        "model2 = ExtraConvModel().to(device)\n",
        "train(model2, train_dataloader)\n",
        "test(model2, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrYvD5js8jha",
        "outputId": "8c5b5034-6552-4ffd-b995-804baca71a5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "DOUBLE EXTRA CONVOLUTION MODEL RESULTS:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 100/100 [06:32<00:00,  3.93s/epoch]\n",
            "Testing: 100%|██████████| 79/79 [00:00<00:00, 213.48batch/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Loss: 1.8627, Test Accuracy: 0.5982\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Train and test the double extra convolution model\n",
        "print()\n",
        "print(\"DOUBLE EXTRA CONVOLUTION MODEL RESULTS:\")\n",
        "model3 = ExtraDoubleConvModel().to(device)\n",
        "train(model3, train_dataloader)\n",
        "test(model3, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCavjLjrvOSI",
        "outputId": "611f36c8-83c3-4c81-97b8-a95ca42804f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "DROPOUT EXTRA CONVOLUTION MODEL RESULTS:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 100/100 [10:32<00:00,  6.32s/epoch]\n",
            "Testing: 100%|██████████| 79/79 [00:00<00:00, 322.33batch/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Loss: 1.8369, Test Accuracy: 0.6217\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Train and test the dropout model\n",
        "print()\n",
        "print(\"DROPOUT EXTRA CONVOLUTION MODEL RESULTS:\")\n",
        "model4 = DropoutModel().to(device)\n",
        "train(model4, train_dataloader)\n",
        "test(model4, test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxstzNRBms7N"
      },
      "source": [
        "# HYPERPARAMETER TUNING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzAQhNYb9-QD"
      },
      "source": [
        "## Batch size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1mI8b9sxumaN"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0YMlgFnmvTb",
        "outputId": "515a001f-8980-40d2-d426-3a59ab376d61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "EXTRA CONVOLUTION MODEL RESULTS:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 100/100 [02:18<00:00,  1.39s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Accuracy: 0.6344\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Bigger batch size\n",
        "batch_size = 256\n",
        "train_split, val_split = random_split(train_dataset, [0.8, 0.2])\n",
        "train_dataloader = DataLoader(train_split, batch_size=batch_size, shuffle=True, pin_memory=True, worker_init_fn=seed_worker)\n",
        "val_dataloader = DataLoader(val_split, batch_size=batch_size, shuffle=True, pin_memory=True, worker_init_fn=seed_worker)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, worker_init_fn=seed_worker)\n",
        "\n",
        "# Train and test the extra convolution model\n",
        "print()\n",
        "print(\"EXTRA CONVOLUTION MODEL RESULTS:\")\n",
        "model_a = ExtraConvModel().to(device)\n",
        "train_with_val(model_a, train_dataloader, val_dataloader)\n",
        "# test(model_a, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AL3XxZFh890w",
        "outputId": "f500e37e-f475-4451-913c-9abfb3725640"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "EXTRA CONVOLUTION MODEL RESULTS:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 100/100 [03:19<00:00,  2.00s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Accuracy: 0.6196\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Smaller batch size\n",
        "batch_size = 64\n",
        "train_split, val_split = random_split(train_dataset, [0.8, 0.2])\n",
        "train_dataloader = DataLoader(train_split, batch_size=batch_size, shuffle=True, pin_memory=True, worker_init_fn=seed_worker)\n",
        "val_dataloader = DataLoader(val_split, batch_size=batch_size, shuffle=True, pin_memory=True, worker_init_fn=seed_worker)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, worker_init_fn=seed_worker)\n",
        "\n",
        "# Train and test the extra convolution model\n",
        "print()\n",
        "print(\"EXTRA CONVOLUTION MODEL RESULTS:\")\n",
        "model_b = ExtraConvModel().to(device)\n",
        "train_with_val(model_b, train_dataloader, val_dataloader)\n",
        "# test(model_b, test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcz9Wiz498TJ"
      },
      "source": [
        "## Learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUXWirZP-TlT",
        "outputId": "0d7dc320-130e-4547-9adb-b2337cca9d39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "EXTRA CONVOLUTION MODEL RESULTS:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 100/100 [02:15<00:00,  1.36s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Accuracy: 0.0971\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Bigger learning rate\n",
        "batch_size = 256\n",
        "train_split, val_split = random_split(train_dataset, [0.8, 0.2])\n",
        "train_dataloader = DataLoader(train_split, batch_size=batch_size, shuffle=True, pin_memory=True, worker_init_fn=seed_worker)\n",
        "val_dataloader = DataLoader(val_split, batch_size=batch_size, shuffle=True, pin_memory=True, worker_init_fn=seed_worker)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, worker_init_fn=seed_worker)\n",
        "\n",
        "# Train and test the extra convolution model\n",
        "print()\n",
        "print(\"EXTRA CONVOLUTION MODEL RESULTS:\")\n",
        "model_c = ExtraConvModel().to(device)\n",
        "train_with_val(model_c, train_dataloader, val_dataloader, lr=0.005)\n",
        "# test(model_c, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhzByUPsCc6z",
        "outputId": "fc769dbf-fd2d-46ba-b4e4-6fdd13c97947"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "EXTRA CONVOLUTION MODEL RESULTS:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 100/100 [02:18<00:00,  1.39s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Accuracy: 0.6396\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# (A little) bigger learning rate\n",
        "batch_size = 256\n",
        "train_split, val_split = random_split(train_dataset, [0.8, 0.2])\n",
        "train_dataloader = DataLoader(train_split, batch_size=batch_size, shuffle=True, pin_memory=True, worker_init_fn=seed_worker)\n",
        "val_dataloader = DataLoader(val_split, batch_size=batch_size, shuffle=True, pin_memory=True, worker_init_fn=seed_worker)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, worker_init_fn=seed_worker)\n",
        "\n",
        "# Train and test the extra convolution model\n",
        "print()\n",
        "print(\"EXTRA CONVOLUTION MODEL RESULTS:\")\n",
        "model_d = ExtraConvModel().to(device)\n",
        "train_with_val(model_d, train_dataloader, val_dataloader, lr=0.002)\n",
        "# test(model_d, test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbbnEXDxumaP"
      },
      "source": [
        "## Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpabUhYfCQ6C",
        "outputId": "83986854-cb57-4ea0-fa1a-39b3757cc370"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "EXTRA CONVOLUTION MODEL RESULTS:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 200/200 [04:38<00:00,  1.39s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Accuracy: 0.6219\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Smaller learning rate\n",
        "batch_size = 256\n",
        "train_split, val_split = random_split(train_dataset, [0.8, 0.2])\n",
        "train_dataloader = DataLoader(train_split, batch_size=batch_size, shuffle=True, pin_memory=True, worker_init_fn=seed_worker)\n",
        "val_dataloader = DataLoader(val_split, batch_size=batch_size, shuffle=True, pin_memory=True, worker_init_fn=seed_worker)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, worker_init_fn=seed_worker)\n",
        "\n",
        "# Train and test the extra convolution model\n",
        "print()\n",
        "print(\"EXTRA CONVOLUTION MODEL RESULTS:\")\n",
        "model_e = ExtraConvModel().to(device)\n",
        "train_with_val(model_e, train_dataloader, val_dataloader, lr=0.0005, epochs=200)\n",
        "# test(model_e, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "My6ChDZMGTyt",
        "outputId": "11e0b9ee-62d6-4ab7-b1d2-601e953cbcdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "EXTRA CONVOLUTION MODEL RESULTS:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 200/200 [04:39<00:00,  1.40s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Accuracy: 0.6290\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Smaller learning rate\n",
        "batch_size = 256\n",
        "train_split, val_split = random_split(train_dataset, [0.8, 0.2])\n",
        "train_dataloader = DataLoader(train_split, batch_size=batch_size, shuffle=True, pin_memory=True, worker_init_fn=seed_worker)\n",
        "val_dataloader = DataLoader(val_split, batch_size=batch_size, shuffle=True, pin_memory=True, worker_init_fn=seed_worker)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, worker_init_fn=seed_worker)\n",
        "\n",
        "# Train and test the extra convolution model\n",
        "print()\n",
        "print(\"EXTRA CONVOLUTION MODEL RESULTS:\")\n",
        "model_f = ExtraConvModel().to(device)\n",
        "train_with_val(model_f, train_dataloader, val_dataloader, lr=0.0008, epochs=200)\n",
        "# test(model_f, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5slxvfnRHnsS",
        "outputId": "c332d845-b50a-4501-e9eb-50ac41575f3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "EXTRA CONVOLUTION MODEL RESULTS:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 200/200 [04:36<00:00,  1.38s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Accuracy: 0.6327\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Smaller learning rate\n",
        "batch_size = 256\n",
        "train_split, val_split = random_split(train_dataset, [0.8, 0.2])\n",
        "train_dataloader = DataLoader(train_split, batch_size=batch_size, shuffle=True, pin_memory=True, worker_init_fn=seed_worker)\n",
        "val_dataloader = DataLoader(val_split, batch_size=batch_size, shuffle=True, pin_memory=True, worker_init_fn=seed_worker)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, worker_init_fn=seed_worker)\n",
        "\n",
        "# Train and test the extra convolution model\n",
        "print()\n",
        "print(\"EXTRA CONVOLUTION MODEL RESULTS:\")\n",
        "model_g = ExtraConvModel().to(device)\n",
        "train_with_val(model_g, train_dataloader, val_dataloader, lr=0.0015, epochs=200)\n",
        "# test(model_g, test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyBZWenNumaP"
      },
      "source": [
        "# Batch normalisation and data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8WycbwLDumaQ"
      },
      "outputs": [],
      "source": [
        "class ExtraConvModelWithBN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # For 32x32 colour images\n",
        "        self.conv_layer = nn.Conv2d(3, 32, (3, 3))\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv_layer_2 = nn.Conv2d(32, 32, (2, 2))\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.max_pool = nn.MaxPool2d((2, 2))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dense = nn.Linear(6272, 128)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "        # Relu after dense\n",
        "        self.output = nn.Linear(128, 10)  # 10 possible categories\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        x = self.conv_layer(x)  # 32x32x3 -> 30x30x32\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv_layer_2(x)  # 30x30x32 -> 29x29x32\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.max_pool(x)  # 14x14x32\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.output(x)\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTX5ZjUsumaQ",
        "outputId": "6d5ae7e9-e0d1-4bc9-8ca0-eef2bc8da2fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "EXTRA CONVOLUTION MODEL RESULTS:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 100/100 [03:06<00:00,  1.87s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Accuracy: 0.6839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# (A little) bigger learning rate\n",
        "batch_size = 256\n",
        "train_split, val_split = random_split(train_dataset, [0.8, 0.2])\n",
        "train_dataloader = DataLoader(train_split, batch_size=batch_size, shuffle=True, pin_memory=True, worker_init_fn=seed_worker)\n",
        "val_dataloader = DataLoader(val_split, batch_size=batch_size, shuffle=True, pin_memory=True, worker_init_fn=seed_worker)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, worker_init_fn=seed_worker)\n",
        "\n",
        "# Train and test the extra convolution model\n",
        "print()\n",
        "print(\"EXTRA CONVOLUTION MODEL RESULTS:\")\n",
        "model_bn = ExtraConvModelWithBN().to(device)\n",
        "train_with_val(model_bn, train_dataloader, val_dataloader, lr=0.002)\n",
        "# test(model_bn, test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lesuaHaumaQ"
      },
      "source": [
        "With both data augmentation and batch normalization: h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6LCd1vMumaQ",
        "outputId": "9555a39d-ea44-459f-f2b8-fd1a04eb0415"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "EXTRA CONVOLUTION MODEL RESULTS:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 100/100 [13:52<00:00,  8.33s/epoch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Accuracy: 0.7256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torchvision.transforms as T\n",
        "from torch.utils.data import ConcatDataset\n",
        "\n",
        "# (A little) bigger learning rate\n",
        "batch_size = 256\n",
        "train_split, val_split = random_split(train_dataset, [0.8, 0.2])\n",
        "\n",
        "train_split = ConcatDataset([\n",
        "    [(T.Compose([\n",
        "        T.RandomHorizontalFlip(),\n",
        "        T.RandomCrop(32, padding=4),\n",
        "    ])(x), y) for x, y in train_split] for _ in range(5)\n",
        "])\n",
        "\n",
        "# train_split = [(T.Compose([\n",
        "#     T.ToTensor(),\n",
        "#     T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "# ])(x), y) for x, y in train_split]\n",
        "\n",
        "train_dataloader = DataLoader(train_split, batch_size=batch_size, shuffle=True, pin_memory=True, worker_init_fn=seed_worker)\n",
        "val_dataloader = DataLoader(val_split, batch_size=batch_size, shuffle=True, pin_memory=True, worker_init_fn=seed_worker)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, worker_init_fn=seed_worker)\n",
        "\n",
        "# Train and test the extra convolution model\n",
        "print()\n",
        "print(\"EXTRA CONVOLUTION MODEL RESULTS:\")\n",
        "model_aug = ExtraConvModelWithBN().to(device)\n",
        "train_with_val(model_aug, train_dataloader, val_dataloader, lr=0.002)\n",
        "# test(model_aug, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOtjUEUcumaR",
        "outputId": "b4c98a8f-5035-4801-c5b5-c504a21f6844"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 40/40 [00:00<00:00, 174.53batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Loss: 1.8276, Test Accuracy: 0.6312\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 40/40 [00:00<00:00, 186.32batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Loss: 1.8464, Test Accuracy: 0.6134\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 40/40 [00:00<00:00, 185.38batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Loss: 2.3606, Test Accuracy: 0.1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 40/40 [00:00<00:00, 193.84batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Loss: 1.8271, Test Accuracy: 0.6325\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 40/40 [00:00<00:00, 203.20batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Loss: 1.8301, Test Accuracy: 0.6319\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 40/40 [00:00<00:00, 208.26batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Loss: 1.8186, Test Accuracy: 0.6389\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 40/40 [00:00<00:00, 217.57batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Loss: 1.8289, Test Accuracy: 0.6292\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 40/40 [00:00<00:00, 183.01batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Loss: 1.7829, Test Accuracy: 0.6791\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 40/40 [00:00<00:00, 180.28batch/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Loss: 1.7335, Test Accuracy: 0.7244\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "test(model_a, test_dataloader)\n",
        "test(model_b, test_dataloader)\n",
        "test(model_c, test_dataloader)\n",
        "test(model_d, test_dataloader)\n",
        "test(model_e, test_dataloader)\n",
        "test(model_f, test_dataloader)\n",
        "test(model_g, test_dataloader)\n",
        "test(model_bn, test_dataloader)\n",
        "test(model_aug, test_dataloader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
